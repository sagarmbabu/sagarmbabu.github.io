<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sagar Murugesh Babu | ML Systems & GPU Software Engineer</title>
  <meta name="description" content="Portfolio of Sagar Murugesh Babu – Software Engineer focused on GPU-accelerated ML systems, CUDA, PyTorch, TensorRT, and inference optimization." />
  <link rel="stylesheet" href="style.css?v=4" />
  <style>
    html { scroll-behavior: smooth; }
  </style>
</head>
<body>

  <!-- Header -->
  <header>
    <h1>Sagar Murugesh Babu</h1>
    <p class="tagline">Software Engineer | ML Systems & GPU Inference Optimization (CUDA • PyTorch • TensorRT • Nsight)</p>
    <nav>
      <a href="#about">About</a>
      <a href="#experience">Experience</a>
      <a href="#projects">Projects</a>
      <a href="#skills">Skills</a>
      <a href="#contact">Contact</a>
    </nav>
  </header>

  <!-- About Section -->
  <section id="about">
    <h2>About Me</h2>
    <p>
      I’m a <strong>Software Engineer</strong> with an <strong>M.S. in Information Technology</strong> from the University of Wisconsin–Milwaukee,
      specializing in <strong>GPU-accelerated machine learning inference</strong> and <strong>high-performance ML systems</strong>.
      My work focuses on reducing latency and improving throughput for transformer-based workloads through CUDA kernel optimization,
      profiling-driven tuning, and optimized inference pipelines.
    </p>
    <p>
      I build and optimize <strong>CUDA kernels</strong> using warp-level primitives, shared memory, and tensor cores, and profile GPU workloads with
      <strong>Nsight Systems</strong> and <strong>Nsight Compute</strong> to identify memory, occupancy, and execution bottlenecks.
      I integrate optimized workflows into <strong>PyTorch</strong> and <strong>TensorRT</strong>, applying techniques like
      <strong>mixed precision (FP16/BF16)</strong>, <strong>kernel fusion</strong>, and <strong>CUDA Graphs</strong> for more consistent real-time inference.
    </p>
    <p>
      I’m actively working on <strong>H100/A100 tuning</strong>, <strong>multi-GPU inference</strong>, and <strong>memory-efficient deployment</strong>
      for large models, and I enjoy collaborating across teams to ship reliable, production-ready systems.
    </p>
  </section>

  <!-- Experience Section -->
  <section id="experience">
    <h2>Experience</h2>
    <div class="projects">

      <div class="card">
        <h3>Research Assistant – University of Wisconsin–Milwaukee</h3>
        <p><strong>Milwaukee, USA</strong> | Jan 2024 – Present</p>
        <ul>
          <li>Developed and optimized GPU-accelerated ML inference pipelines using <strong>CUDA</strong> and <strong>PyTorch</strong>.</li>
          <li>Designed high-performance CUDA kernels leveraging <strong>warp-level primitives</strong>, <strong>shared memory</strong>, and <strong>tensor cores</strong> on NVIDIA GPUs.</li>
          <li>Profiled GPU workloads with <strong>Nsight Systems</strong> and <strong>Nsight Compute</strong> to identify bottlenecks in memory throughput, occupancy, and execution.</li>
          <li>Applied <strong>mixed-precision (FP16/BF16)</strong> optimization using Tensor Cores to improve latency and throughput under high batch-load inference.</li>
          <li>Integrated optimized kernels into <strong>PyTorch</strong> inference flows and benchmarked performance with <strong>TensorRT</strong> for transformer workloads.</li>
          <li>Implemented <strong>kernel fusion</strong> and <strong>CUDA Graph capture</strong> to reduce kernel launch overhead and improve inference consistency across dynamic shapes.</li>
          <li>Optimized transformer attention/MLP paths and supported <strong>multi-GPU inference</strong> and memory-efficient deployment for long-context scenarios.</li>
        </ul>
      </div>

      <div class="card">
        <h3>Software Engineer – Technotharanga</h3>
        <p><strong>India</strong> | Oct 2022 – Dec 2023</p>
        <ul>
          <li>Built a drag-and-drop personalization tool to reorder gift cards based on user interests, increasing gift card conversion rates by approximately 10%.</li>
          <li>Designed and developed a dashboard builder framework using a <strong>microfrontend architecture</strong>, enabling faster and more efficient creation of new dashboards.</li>
          <li>Implemented a comprehensive <strong>role-based permission management system</strong> to control user access levels across applications.</li>
          <li>Developed a common <strong>authentication and authorization mechanism</strong> to simplify secure access and reduce duplication across services.</li>
          <li>Built and maintained <strong>backend APIs</strong> using Node.js and Java in a distributed, service-oriented environment.</li>
          <li>Collaborated closely with frontend teams using <strong>React and Redux</strong> to deliver scalable, responsive user interfaces.</li>
          <li>Containerized services using <strong>Docker</strong> to support consistent development, testing, and deployment workflows.</li>
          <li>Participated in debugging, performance optimization, and documentation to ensure stable releases and long-term maintainability.</li>
        </ul>
      </div>

    </div>
  </section>

  <!-- Projects Section -->
  <section id="projects">
    <h2>Projects</h2>
    <div class="projects">

      <div class="card">
        <h3>Hybrid JIT–CUDA Graph Runtime for LLM Inference</h3>
        <p>
          Designed a hybrid runtime combining static <strong>CUDA Graph replay</strong> with <strong>JIT-compiled dynamic kernels</strong>
          to enable low-latency, deterministic autoregressive inference.
        </p>
        <ul>
          <li>Reduced Python-side kernel dispatch by executing transformer blocks within CUDA Graphs while handling dynamic KV-cache growth.</li>
          <li>Implemented persistent memory preallocation and graph-safe KV-cache indexing to maintain pointer stability under dynamic sequences.</li>
          <li>Benchmarked against PyTorch Eager and TensorRT-LLM, achieving significant improvements in TTFT and tail latency on NVIDIA GPUs.</li>
        </ul>
      </div>

      <div class="card">
        <h3>GPU Performance Optimization</h3>
        <p>
          Designed and benchmarked custom CUDA kernels to enhance deep learning pipeline performance using profiling-driven optimization.
        </p>
        <ul>
          <li>Applied warp-level parallelism to maximize SM utilization and reduce idle threads.</li>
          <li>Implemented mixed-precision (FP16/BF16) to accelerate inference and reduce memory footprint.</li>
          <li>Improved memory access patterns with coalescing to reduce global memory latency.</li>
        </ul>
      </div>

      <div class="card">
        <h3>TensorRT Inference Optimization (FP16)</h3>
        <p>
          Converted ResNet models to TensorRT and applied FP16 optimizations using Tensor Cores for faster inference.
        </p>
        <ul>
          <li>Reduced batch inference latency through kernel fusion and optimized memory transfers.</li>
          <li>Validated accuracy stayed within acceptable range post-optimization.</li>
        </ul>
      </div>

    </div>
  </section>

  <!-- Skills Section -->
  <section id="skills">
    <h2>Skills</h2>
    <div class="projects">
      <div class="card">
        <h3>Core</h3>
        <p>CUDA • C++ • Python • PyTorch • TensorRT • Nsight Systems/Compute • Mixed Precision (FP16/BF16) • Kernel Fusion • CUDA Graphs</p>
      </div>
      <div class="card">
        <h3>Systems & ML</h3>
        <p>Multi-GPU Inference • cuBLAS • NCCL • Performance Profiling • Memory Optimization • Transformer Inference</p>
      </div>
      <div class="card">
        <h3>Foundations</h3>
        <p>Data Structures • Operating Systems • Databases/DBMS • Computer Networks • NLP • Algorithms</p>
      </div>
    </div>
  </section>

  <!-- Contact Section -->
  <section id="contact">
    <h2>Contact</h2>
    <p>Email: <a href="mailto:muruges3@uwm.edu">muruges3@uwm.edu</a></p>
    <p>LinkedIn: <a href="https://www.linkedin.com/in/sagarmbabu/" target="_blank" rel="noopener">linkedin.com/in/sagarmbabu</a></p>
    <p>GitHub: <a href="https://github.com/sagarmbabu" target="_blank" rel="noopener">github.com/sagarmbabu</a></p>
  </section>

  <!-- Footer -->
  <footer>
    <p>© 2025 Sagar Murugesh Babu | Built with GitHub Pages</p>
  </footer>

</body>
</html>
